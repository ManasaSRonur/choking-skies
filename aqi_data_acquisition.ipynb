{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Goal\n",
    "\n",
    "The objective of this notebook is to collect historical Air Quality Index (AQI) data for [Madison](https://docs.google.com/spreadsheets/d/1pHLA9XzXoy9nJTaiNkgThGPQjVEa0tfeH203I6FA238/edit?gid=0#gid=0&range=E55), located in [Dane County](https://en.wikipedia.org/wiki/Dane_County,_Wisconsin), [Wisconsin](https://docs.google.com/spreadsheets/d/1pHLA9XzXoy9nJTaiNkgThGPQjVEa0tfeH203I6FA238/edit?gid=0#gid=0&range=F55) starting from 1964 to 2024, for dates ranging from May 1st to October 31st.\n",
    "\n",
    "We utilize the US Environmental Protection Agency (EPA) Air Quality System (AQS) API, which focuses on historical data rather than real-time air quality information. The API [documentation](https://aqs.epa.gov/aqsweb/documents/data_api.html) provides detailed explanations of the various parameters used in API calls and includes examples of how to retrieve data. For more information about the Air Quality System, refer to the [EPA FAQ](https://www.epa.gov/outdoor-air-quality-data/frequent-questions-about-airdata).\n",
    "\n",
    "The Air Quality Index measures the air quality on a given day, indicating how safe or polluted the air is. It tracks common pollutants such as smog, smoke and carbon monoxide. A rating between 0 and 50 reflects clean, healthy air, while the index value of 301 or above represents hazardous conditions. A comprehensive explanation of AQI calculation is available [here](https://www.airnow.gov/aqi/aqi-basics/).\n",
    "\n",
    "The US EPA was created in the early 1970's. The EPA reports that they only started broad based monitoring with standardized quality assurance procedures in the 1980's. Many counties will have data starting somewhere between 1983 and 1988. However, some counties still do not have any air quality monitoring stations. The API helps resolve this by providing calls to search for monitoring stations and data using either station ids, or a county designation or a geographic bounding box.\n",
    "\n",
    "To locate nearby air quality monitoring stations, Federal Information Processing Series (FIPS) codes are required for the specific city, county, and state. The FIPS data for this notebook was sourced from [federal communications commission](https://transition.fcc.gov/oet/info/maps/census/fips/fips.txt). I am using the county level FIPS code for Madison.\n",
    "\n",
    "State FIPS: 55\n",
    "County FIPS: 025\n",
    "County level FIPS: 55025\n",
    "\n",
    "\n",
    "### License\n",
    "\n",
    "#### Code Attribution\n",
    "\n",
    "Snippets of the code were taken from a code example developed by **Dr. David W. McDonald** for use in DATA 512, a course in the UW MS Data Science degree program. This code is provided under the [**Creative Commons CC-BY license**](https://creativecommons.org/licenses/by/4.0/).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 1: Data Retrieval"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importing essential libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \n",
    "#    These are standard python modules\n",
    "#\n",
    "#import json, time, urllib.parse\n",
    "import json, time\n",
    "#    The 'requests' module is a distribution module for making web requests. If you do not have it already, you'll need to install it\n",
    "import requests\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "import warnings\n",
    "\n",
    "from pyproj import Transformer, Geod\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Defining global constants that will used during API calls and data filtering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#########\n",
    "#\n",
    "#    CONSTANTS\n",
    "#\n",
    "\n",
    "#\n",
    "#    This is the root of all AQS API URLs\n",
    "#\n",
    "API_REQUEST_URL = 'https://aqs.epa.gov/data/api'\n",
    "\n",
    "#\n",
    "#    These are some of the 'actions' we can ask the API to take or requests that we can make of the API\n",
    "#\n",
    "#    Sign-up request - generally only performed once - unless you lose your key\n",
    "API_ACTION_SIGNUP = '/signup?email={email}'\n",
    "#\n",
    "#    List actions provide information on API parameter values that are required by some other actions/requests\n",
    "API_ACTION_LIST_CLASSES = '/list/classes?email={email}&key={key}'\n",
    "API_ACTION_LIST_PARAMS = '/list/parametersByClass?email={email}&key={key}&pc={pclass}'\n",
    "API_ACTION_LIST_SITES = '/list/sitesByCounty?email={email}&key={key}&state={state}&county={county}'\n",
    "#\n",
    "#    Monitor actions are requests for monitoring stations that meet specific criteria\n",
    "API_ACTION_MONITORS_COUNTY = '/monitors/byCounty?email={email}&key={key}&param={param}&bdate={begin_date}&edate={end_date}&state={state}&county={county}'\n",
    "API_ACTION_MONITORS_BOX = '/monitors/byBox?email={email}&key={key}&param={param}&bdate={begin_date}&edate={end_date}&minlat={minlat}&maxlat={maxlat}&minlon={minlon}&maxlon={maxlon}'\n",
    "#\n",
    "#    Summary actions are requests for summary data. These are for daily summaries\n",
    "API_ACTION_DAILY_SUMMARY_COUNTY = '/dailyData/byCounty?email={email}&key={key}&param={param}&bdate={begin_date}&edate={end_date}&state={state}&county={county}'\n",
    "API_ACTION_DAILY_SUMMARY_BOX = '/dailyData/byBox?email={email}&key={key}&param={param}&bdate={begin_date}&edate={end_date}&minlat={minlat}&maxlat={maxlat}&minlon={minlon}&maxlon={maxlon}'\n",
    "#\n",
    "#    It is always nice to be respectful of a free data resource.\n",
    "#    We're going to observe a 100 requests per minute limit - which is fairly nice\n",
    "API_LATENCY_ASSUMED = 0.002       # Assuming roughly 2ms latency on the API and network\n",
    "API_THROTTLE_WAIT = (1.0/100.0)-API_LATENCY_ASSUMED\n",
    "#\n",
    "#\n",
    "#    This is a template that covers most of the parameters for the actions we might take, from the set of actions\n",
    "#    above. In the examples below, most of the time parameters can either be supplied as individual values to a\n",
    "#    function - or they can be set in a copy of the template and passed in with the template.\n",
    "# \n",
    "AQS_REQUEST_TEMPLATE = {\n",
    "    \"email\":      \"manasars@uw.edu\",     \n",
    "    \"key\":        \"\",      \n",
    "    \"state\":      \"55\",     # the two digit state FIPS # as a string\n",
    "    \"county\":     \"025\",     # the three digit county FIPS # as a string\n",
    "    \"begin_date\": \"19640501\",     # the start of a time window in YYYYMMDD format\n",
    "    \"end_date\":   \"20211031\",     # the end of a time window in YYYYMMDD format, begin_date and end_date must be in the same year\n",
    "    \"minlat\":    0.0,\n",
    "    \"maxlat\":    0.0,\n",
    "    \"minlon\":    0.0,\n",
    "    \"maxlon\":    0.0,\n",
    "    \"param\":     \"\",     # a list of comma separated 5 digit codes, max 5 codes requested\n",
    "    \"pclass\":    \"\"      # parameter class is only used by the List calls\n",
    "}\n",
    "\n",
    "USERNAME = \"manasars@uw.edu\"\n",
    "APIKEY = \"cobaltgoose48\"\n",
    "\n",
    "STARTYEAR = 1964\n",
    "ENDYEAR = 2024"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we can use the API we need to request a key. I am using my email address to make the request. The EPA then sends a confirmation email link and a 'key' that we use for all other requests.\n",
    "We only need to sign-up once, unless we want to invalidate the current key (by getting a new key) or we lose the key. So this block of code has commented out as the key has been already generated. In case there is a need for generating it again, uncomment the entire cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #\n",
    "# #    This implements the sign-up request. The parameters are standardized so that this function definition matches\n",
    "# #    all of the others. However, the easiest way to call this is to simply call this function with your preferred\n",
    "# #    email address.\n",
    "# #\n",
    "# def request_signup(email_address = None,\n",
    "#                    endpoint_url = API_REQUEST_URL, \n",
    "#                    endpoint_action = API_ACTION_SIGNUP, \n",
    "#                    request_template = AQS_REQUEST_TEMPLATE,\n",
    "#                    headers = None):\n",
    "    \n",
    "#     # Make sure we have a string - if you don't have access to this email addres, things might go badly for you\n",
    "#     if email_address:\n",
    "#         request_template['email'] = email_address        \n",
    "    \n",
    "#     if not request_template['email']: \n",
    "#         raise Exception(\"Must supply an email address to call 'request_signup()'\")\n",
    "\n",
    "#     if '@' not in request_template['email']: \n",
    "#         raise Exception(f\"Must supply an email address to call 'request_signup()'. The string '{request_template['email']}' does not look like an email address.\")\n",
    "\n",
    "#     # Compose the signup url - create a request URL by combining the endpoint_url with the parameters for the request\n",
    "#     request_url = endpoint_url+endpoint_action.format(**request_template)\n",
    "        \n",
    "#     # make the request\n",
    "#     try:\n",
    "#         # Wait first, to make sure we don't exceed a rate limit in the situation where an exception occurs\n",
    "#         # during the request processing - throttling is always a good practice with a free data source\n",
    "#         if API_THROTTLE_WAIT > 0.0:\n",
    "#             time.sleep(API_THROTTLE_WAIT)\n",
    "#         response = requests.get(request_url, headers=headers)\n",
    "#         json_response = response.json()\n",
    "#     except Exception as e:\n",
    "#         print(e)\n",
    "#         json_response = None\n",
    "#     return json_response\n",
    "\n",
    "# print(\"Requesting SIGNUP ...\")\n",
    "# response = request_signup(\"manasars@uw.edu\")\n",
    "# print(json.dumps(response,indent=4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we have a key, the next thing is to get information about the different types of air quality monitoring (sensors) and the different places where we might find air quality stations. The monitoring system is complex and changes all the time. The EPA implementation allows an API user to find changes to monitoring sites and sensors by making requests - maybe monthly, or daily. This API approach is probably better than having the EPA publish documentation that may be out of date as soon as it hits a web page. Some of the responses rely on jargon or terms-of-art. So, one needs to know a bit about the way atmospheric sciece works to understand some of the terms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "#    This implements the list request. There are several versions of the list request that only require email and key.\n",
    "#    This code sets the default action/requests to list the groups or parameter class descriptors. Having those descriptors \n",
    "#    allows one to request the individual (proprietary) 5 digit codes for individual air quality measures by using the\n",
    "#    param request. Some code in later cells will illustrate those requests.\n",
    "#\n",
    "def request_list_info(email_address = None, key = None,\n",
    "                      endpoint_url = API_REQUEST_URL, \n",
    "                      endpoint_action = API_ACTION_LIST_CLASSES, \n",
    "                      request_template = AQS_REQUEST_TEMPLATE,\n",
    "                      headers = None):\n",
    "    \n",
    "    #  Make sure we have email and key - at least\n",
    "    #  This prioritizes the info from the call parameters - not what's already in the template\n",
    "    if email_address:\n",
    "        request_template['email'] = email_address\n",
    "    if key:\n",
    "        request_template['key'] = key\n",
    "    \n",
    "    # For the basic request we need an email address and a key\n",
    "    if not request_template['email']:\n",
    "        raise Exception(\"Must supply an email address to call 'request_list_info()'\")\n",
    "    if not request_template['key']: \n",
    "        raise Exception(\"Must supply a key to call 'request_list_info()'\")\n",
    "\n",
    "    # compose the request\n",
    "    request_url = endpoint_url+endpoint_action.format(**request_template)\n",
    "        \n",
    "    # make the request\n",
    "    try:\n",
    "        # Wait first, to make sure we don't exceed a rate limit in the situation where an exception occurs\n",
    "        # during the request processing - throttling is always a good practice with a free data source\n",
    "        if API_THROTTLE_WAIT > 0.0:\n",
    "            time.sleep(API_THROTTLE_WAIT)\n",
    "        response = requests.get(request_url, headers=headers)\n",
    "        json_response = response.json()\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        json_response = None\n",
    "    return json_response\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\n",
      "    {\n",
      "        \"code\": \"AIRNOW MAPS\",\n",
      "        \"value_represented\": \"The parameters represented on AirNow maps (88101, 88502, and 44201)\"\n",
      "    },\n",
      "    {\n",
      "        \"code\": \"ALL\",\n",
      "        \"value_represented\": \"Select all Parameters Available\"\n",
      "    },\n",
      "    {\n",
      "        \"code\": \"AQI POLLUTANTS\",\n",
      "        \"value_represented\": \"Pollutants that have an AQI Defined\"\n",
      "    },\n",
      "    {\n",
      "        \"code\": \"CORE_HAPS\",\n",
      "        \"value_represented\": \"Urban Air Toxic Pollutants\"\n",
      "    },\n",
      "    {\n",
      "        \"code\": \"CRITERIA\",\n",
      "        \"value_represented\": \"Criteria Pollutants\"\n",
      "    },\n",
      "    {\n",
      "        \"code\": \"CSN DART\",\n",
      "        \"value_represented\": \"List of CSN speciation parameters to populate the STI DART tool\"\n",
      "    },\n",
      "    {\n",
      "        \"code\": \"FORECAST\",\n",
      "        \"value_represented\": \"Parameters routinely extracted by AirNow (STI)\"\n",
      "    },\n",
      "    {\n",
      "        \"code\": \"HAPS\",\n",
      "        \"value_represented\": \"Hazardous Air Pollutants\"\n",
      "    },\n",
      "    {\n",
      "        \"code\": \"IMPROVE CARBON\",\n",
      "        \"value_represented\": \"IMPROVE Carbon Parameters\"\n",
      "    },\n",
      "    {\n",
      "        \"code\": \"IMPROVE_SPECIATION\",\n",
      "        \"value_represented\": \"PM2.5 Speciated Parameters Measured at IMPROVE sites\"\n",
      "    },\n",
      "    {\n",
      "        \"code\": \"MET\",\n",
      "        \"value_represented\": \"Meteorological Parameters\"\n",
      "    },\n",
      "    {\n",
      "        \"code\": \"NATTS CORE HAPS\",\n",
      "        \"value_represented\": \"The core list of toxics of interest to the NATTS program.\"\n",
      "    },\n",
      "    {\n",
      "        \"code\": \"NATTS REQUIRED\",\n",
      "        \"value_represented\": \"Required compounds to be collected in the National Air Toxics Network\"\n",
      "    },\n",
      "    {\n",
      "        \"code\": \"PAMS\",\n",
      "        \"value_represented\": \"Photochemical Assessment Monitoring System\"\n",
      "    },\n",
      "    {\n",
      "        \"code\": \"PAMS_VOC\",\n",
      "        \"value_represented\": \"Volatile Organic Compound subset of the PAMS Parameters\"\n",
      "    },\n",
      "    {\n",
      "        \"code\": \"PM COARSE\",\n",
      "        \"value_represented\": \"PM between 2.5 and 10 micrometers\"\n",
      "    },\n",
      "    {\n",
      "        \"code\": \"PM10 SPECIATION\",\n",
      "        \"value_represented\": \"PM10 Speciated Parameters\"\n",
      "    },\n",
      "    {\n",
      "        \"code\": \"PM2.5 CONT NONREF\",\n",
      "        \"value_represented\": \"PM2.5 Continuous, Nonreference Methods\"\n",
      "    },\n",
      "    {\n",
      "        \"code\": \"PM2.5 MASS/QA\",\n",
      "        \"value_represented\": \"PM2.5 Mass and QA Parameters\"\n",
      "    },\n",
      "    {\n",
      "        \"code\": \"SCHOOL AIR TOXICS\",\n",
      "        \"value_represented\": \"School Air Toxics Program Parameters\"\n",
      "    },\n",
      "    {\n",
      "        \"code\": \"SPECIATION\",\n",
      "        \"value_represented\": \"PM2.5 Speciated Parameters\"\n",
      "    },\n",
      "    {\n",
      "        \"code\": \"SPECIATION CARBON\",\n",
      "        \"value_represented\": \"PM2.5 Speciation Carbon Parameters\"\n",
      "    },\n",
      "    {\n",
      "        \"code\": \"SPECIATION CATION/ANION\",\n",
      "        \"value_represented\": \"PM2.5 Speciation Cation/Anion Parameters\"\n",
      "    },\n",
      "    {\n",
      "        \"code\": \"SPECIATION METALS\",\n",
      "        \"value_represented\": \"PM2.5 Speciation Metal Parameters\"\n",
      "    },\n",
      "    {\n",
      "        \"code\": \"UATMP CARBONYL\",\n",
      "        \"value_represented\": \"Urban Air Toxics Monitoring Program Carbonyls\"\n",
      "    },\n",
      "    {\n",
      "        \"code\": \"UATMP VOC\",\n",
      "        \"value_represented\": \"Urban Air Toxics Monitoring Program VOCs\"\n",
      "    },\n",
      "    {\n",
      "        \"code\": \"VOC\",\n",
      "        \"value_represented\": \"Volatile organic compounds\"\n",
      "    }\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "#   The default should get us a list of the various groups or classes of sensors. These classes are user defined names for clustors of\n",
    "#   sensors that might be part of a package or default air quality sensing station. We need a class name to start getting down to the\n",
    "#   a sensor ID. Each sensor type has an ID number. We'll eventually need those ID numbers to be able to request values that come from\n",
    "#   that specific sensor.\n",
    "#\n",
    "request_data = AQS_REQUEST_TEMPLATE.copy()\n",
    "request_data['email'] = USERNAME\n",
    "request_data['key'] = APIKEY\n",
    "\n",
    "response = request_list_info(request_template=request_data)\n",
    "\n",
    "if response[\"Header\"][0]['status'] == \"Success\":\n",
    "    print(json.dumps(response['Data'],indent=4))\n",
    "else:\n",
    "    print(json.dumps(response,indent=4))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are interested in getting to something that might be the Air Quality Index (AQI). AQI is reported on the news often around smoke and smog values. The AQI is a complex measure of different gasses and of the particles in the air (dust, dirt, ash etc). From the list produced by our 'list/Classes' request above, it looks like there is a class of sensors called \"AQI POLLUTANTS\". Let's try to get a list of those specific sensors and see what we can get from those."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "#   Once we have a list of the classes or groups of possible sensors, we can find the sensor IDs that make up that class (group)\n",
    "#   The one that looks to be associated with the Air Quality Index is \"AQI POLLUTANTS\"\n",
    "#   We'll use that to make another list request.\n",
    "#\n",
    "AQI_PARAM_CLASS = \"AQI POLLUTANTS\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\n",
      "    {\n",
      "        \"code\": \"42101\",\n",
      "        \"value_represented\": \"Carbon monoxide\"\n",
      "    },\n",
      "    {\n",
      "        \"code\": \"42401\",\n",
      "        \"value_represented\": \"Sulfur dioxide\"\n",
      "    },\n",
      "    {\n",
      "        \"code\": \"42602\",\n",
      "        \"value_represented\": \"Nitrogen dioxide (NO2)\"\n",
      "    },\n",
      "    {\n",
      "        \"code\": \"44201\",\n",
      "        \"value_represented\": \"Ozone\"\n",
      "    },\n",
      "    {\n",
      "        \"code\": \"81102\",\n",
      "        \"value_represented\": \"PM10 Total 0-10um STP\"\n",
      "    },\n",
      "    {\n",
      "        \"code\": \"88101\",\n",
      "        \"value_represented\": \"PM2.5 - Local Conditions\"\n",
      "    },\n",
      "    {\n",
      "        \"code\": \"88502\",\n",
      "        \"value_represented\": \"Acceptable PM2.5 AQI & Speciation Mass\"\n",
      "    }\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "#   Structure a request to get the sensor IDs associated with the AQI\n",
    "#\n",
    "request_data = AQS_REQUEST_TEMPLATE.copy()\n",
    "request_data['email'] = USERNAME\n",
    "request_data['key'] = APIKEY\n",
    "request_data['pclass'] = AQI_PARAM_CLASS  # here we specify that we want this 'pclass' or parameter classs\n",
    "\n",
    "response = request_list_info(request_template=request_data, endpoint_action=API_ACTION_LIST_PARAMS)\n",
    "\n",
    "if response[\"Header\"][0]['status'] == \"Success\":\n",
    "    print(json.dumps(response['Data'],indent=4))\n",
    "else:\n",
    "    print(json.dumps(response,indent=4))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now have a response containing a set of sensor ID numbers. The list includes the sensor numbers as well as a description or name for each sensor."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The EPA AQS API has limits on some call parameters. Specifically, when we request data for sensors we can only specify a maximum of 5 different sensor values to return. This means we cannot get all of the Air Quality Index parameters in one request for data. We have to break it up. So we break the request into two logical groups, the AQI sensors that sample gasses and the AQI sensors that sample particles in the air."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "#   Given the set of sensor codes, now we can create a parameter list or 'param' value as defined by the AQS API spec.\n",
    "#   It turns out that we want all of these measures for AQI, but we need to have two different param constants to get\n",
    "#   all seven of the code types. We can only have a max of 5 sensors/values request per param.\n",
    "#\n",
    "#   Gaseous AQI pollutants CO, SO2, NO2, and O2\n",
    "AQI_PARAMS_GASEOUS = \"42101,42401,42602,44201\"\n",
    "#\n",
    "#   Particulate AQI pollutants PM10, PM2.5, and Acceptable PM2.5\n",
    "AQI_PARAMS_PARTICULATES = \"81102,88101,88502\"\n",
    "#   \n",
    "#"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Air quality monitoring stations are located all over the US at different locations. To have AQI data relevant to Madison, we must focus on monitoring stations in and around Madison.\n",
    "To do so, we must supply the FIPS number for the state and county as a 5 digit string. This format, the 5 digit string, is a 'old' format that is still widely used. There are new codes that may eventually be adopted for the US government information systems. But FIPS is currently what the AQS uses, so that's what is in the list as the constant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "#   We'll use these the Madison city location\n",
    "#\n",
    "CITY_LOCATIONS = {\n",
    "    'madison' :       {'city'   : 'Madison',\n",
    "                       'county' : 'Dane',\n",
    "                       'state'  : 'Wisconsin',\n",
    "                       'fips'   : '55025',\n",
    "                       'latlon' : [43.074722,-89.384167] }\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given our CITY_LOCATIONS constant we can now find which monitoring locations are nearby. One option is to use the county to define the area we are interest in. You can get the EPA to list their monitoring stations by county. We can also get a set of monitoring stations by using a bounding box of latitude, longitude points. But I am using the county approach."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\n",
      "    {\n",
      "        \"code\": \"0001\",\n",
      "        \"value_represented\": null\n",
      "    },\n",
      "    {\n",
      "        \"code\": \"0002\",\n",
      "        \"value_represented\": null\n",
      "    },\n",
      "    {\n",
      "        \"code\": \"0003\",\n",
      "        \"value_represented\": null\n",
      "    },\n",
      "    {\n",
      "        \"code\": \"0004\",\n",
      "        \"value_represented\": null\n",
      "    },\n",
      "    {\n",
      "        \"code\": \"0005\",\n",
      "        \"value_represented\": null\n",
      "    },\n",
      "    {\n",
      "        \"code\": \"0006\",\n",
      "        \"value_represented\": null\n",
      "    },\n",
      "    {\n",
      "        \"code\": \"0007\",\n",
      "        \"value_represented\": null\n",
      "    },\n",
      "    {\n",
      "        \"code\": \"0008\",\n",
      "        \"value_represented\": null\n",
      "    },\n",
      "    {\n",
      "        \"code\": \"0009\",\n",
      "        \"value_represented\": null\n",
      "    },\n",
      "    {\n",
      "        \"code\": \"0010\",\n",
      "        \"value_represented\": null\n",
      "    },\n",
      "    {\n",
      "        \"code\": \"0011\",\n",
      "        \"value_represented\": \"LOCATED IN LOT #14, CITY OF FITCHBURG COMMERCIAL PARK\"\n",
      "    },\n",
      "    {\n",
      "        \"code\": \"0012\",\n",
      "        \"value_represented\": null\n",
      "    },\n",
      "    {\n",
      "        \"code\": \"0013\",\n",
      "        \"value_represented\": null\n",
      "    },\n",
      "    {\n",
      "        \"code\": \"0014\",\n",
      "        \"value_represented\": null\n",
      "    },\n",
      "    {\n",
      "        \"code\": \"0015\",\n",
      "        \"value_represented\": \"TOWN OF VIENNA\"\n",
      "    },\n",
      "    {\n",
      "        \"code\": \"0016\",\n",
      "        \"value_represented\": \"MAINTENANCE BUILDING\"\n",
      "    },\n",
      "    {\n",
      "        \"code\": \"0017\",\n",
      "        \"value_represented\": null\n",
      "    },\n",
      "    {\n",
      "        \"code\": \"0018\",\n",
      "        \"value_represented\": null\n",
      "    },\n",
      "    {\n",
      "        \"code\": \"0019\",\n",
      "        \"value_represented\": null\n",
      "    },\n",
      "    {\n",
      "        \"code\": \"0020\",\n",
      "        \"value_represented\": null\n",
      "    },\n",
      "    {\n",
      "        \"code\": \"0021\",\n",
      "        \"value_represented\": null\n",
      "    },\n",
      "    {\n",
      "        \"code\": \"0022\",\n",
      "        \"value_represented\": null\n",
      "    },\n",
      "    {\n",
      "        \"code\": \"0023\",\n",
      "        \"value_represented\": null\n",
      "    },\n",
      "    {\n",
      "        \"code\": \"0024\",\n",
      "        \"value_represented\": null\n",
      "    },\n",
      "    {\n",
      "        \"code\": \"0025\",\n",
      "        \"value_represented\": \"DAYTON RESERVOIR SITE\"\n",
      "    },\n",
      "    {\n",
      "        \"code\": \"0026\",\n",
      "        \"value_represented\": null\n",
      "    },\n",
      "    {\n",
      "        \"code\": \"0027\",\n",
      "        \"value_represented\": null\n",
      "    },\n",
      "    {\n",
      "        \"code\": \"0031\",\n",
      "        \"value_represented\": null\n",
      "    },\n",
      "    {\n",
      "        \"code\": \"0032\",\n",
      "        \"value_represented\": null\n",
      "    },\n",
      "    {\n",
      "        \"code\": \"0033\",\n",
      "        \"value_represented\": null\n",
      "    },\n",
      "    {\n",
      "        \"code\": \"0034\",\n",
      "        \"value_represented\": null\n",
      "    },\n",
      "    {\n",
      "        \"code\": \"0035\",\n",
      "        \"value_represented\": null\n",
      "    },\n",
      "    {\n",
      "        \"code\": \"0036\",\n",
      "        \"value_represented\": null\n",
      "    },\n",
      "    {\n",
      "        \"code\": \"0037\",\n",
      "        \"value_represented\": \"RODEFELD LANDFILL\"\n",
      "    },\n",
      "    {\n",
      "        \"code\": \"0038\",\n",
      "        \"value_represented\": \"ON COUNTY AB, APPROX 1 MILE NORTH OF STH 12/18\"\n",
      "    },\n",
      "    {\n",
      "        \"code\": \"0039\",\n",
      "        \"value_represented\": \"DOES NOT OPERATE DURING WINTER MONTHS\"\n",
      "    },\n",
      "    {\n",
      "        \"code\": \"0041\",\n",
      "        \"value_represented\": \"MADISON EAST\"\n",
      "    },\n",
      "    {\n",
      "        \"code\": \"0042\",\n",
      "        \"value_represented\": \"TRAILER ON EAST CORNER OF EAST WASHINGTON AND FIRST ST\"\n",
      "    },\n",
      "    {\n",
      "        \"code\": \"0043\",\n",
      "        \"value_represented\": \"ON PLATFORM 12 MILES NE OF DEAD END OF S MARQUETTE ST\"\n",
      "    },\n",
      "    {\n",
      "        \"code\": \"0044\",\n",
      "        \"value_represented\": \"DESCRIBES POINT OF CENTER OF PLATFORM ON WHICH SAMPLERS ARE LOCATED\"\n",
      "    },\n",
      "    {\n",
      "        \"code\": \"0045\",\n",
      "        \"value_represented\": \"MADISON PRAIRIE LANDFILL\"\n",
      "    },\n",
      "    {\n",
      "        \"code\": \"0046\",\n",
      "        \"value_represented\": \"MADISON PRAIRIE LANDFILL\"\n",
      "    },\n",
      "    {\n",
      "        \"code\": \"0047\",\n",
      "        \"value_represented\": \"MADISON - UNIVERSITY AVE WELL #6\"\n",
      "    },\n",
      "    {\n",
      "        \"code\": \"0048\",\n",
      "        \"value_represented\": null\n",
      "    },\n",
      "    {\n",
      "        \"code\": \"0898\",\n",
      "        \"value_represented\": null\n",
      "    },\n",
      "    {\n",
      "        \"code\": \"0993\",\n",
      "        \"value_represented\": null\n",
      "    },\n",
      "    {\n",
      "        \"code\": \"0994\",\n",
      "        \"value_represented\": null\n",
      "    },\n",
      "    {\n",
      "        \"code\": \"0995\",\n",
      "        \"value_represented\": null\n",
      "    },\n",
      "    {\n",
      "        \"code\": \"0997\",\n",
      "        \"value_represented\": null\n",
      "    },\n",
      "    {\n",
      "        \"code\": \"0998\",\n",
      "        \"value_represented\": null\n",
      "    },\n",
      "    {\n",
      "        \"code\": \"0999\",\n",
      "        \"value_represented\": null\n",
      "    },\n",
      "    {\n",
      "        \"code\": \"1005\",\n",
      "        \"value_represented\": \"Charter St HP (CSHP)\"\n",
      "    }\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "#\n",
    "#  This list request should give us a list of all the monitoring stations in the county specified by the\n",
    "#  given city\n",
    "#\n",
    "request_data = AQS_REQUEST_TEMPLATE.copy()\n",
    "request_data['email'] = USERNAME\n",
    "request_data['key'] = APIKEY\n",
    "request_data['state'] = CITY_LOCATIONS['madison']['fips'][:2]   # the first two digits (characters) of FIPS is the state code\n",
    "request_data['county'] = CITY_LOCATIONS['madison']['fips'][2:]  # the last three digits (characters) of FIPS is the county code\n",
    "\n",
    "response = request_list_info(request_template=request_data, endpoint_action=API_ACTION_LIST_SITES)\n",
    "\n",
    "if response[\"Header\"][0]['status'] == \"Success\":\n",
    "    print(json.dumps(response['Data'],indent=4))\n",
    "else:\n",
    "    print(json.dumps(response,indent=4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above response gives us a list of monitoring stations. Each monitoring station has a unique \"code\" which is a string number, and, sometimes, a description. The description seems to be something about where the monitoring station is located. Since we have many monitoring stations in Dane county, we can skip using bounding box approach."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function below is designed to encapsulate requests to the EPA AQS API. When calling the function we should create/copy a parameter template, then initialize that template with values that won't change with each call. Then on each call simply pass in the parameters that need to change, like date ranges."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "#    This implements the daily summary request. Daily summary provides a daily summary value for each sensor being requested\n",
    "#    from the start date to the end date. \n",
    "#\n",
    "#    Like the two other functions, this can be called with a mixture of a defined parameter dictionary, or with function\n",
    "#    parameters. If function parameters are provided, those take precedence over any parameters from the request template.\n",
    "#\n",
    "def request_daily_summary(email_address = None, key = None, param=None,\n",
    "                          begin_date = None, end_date = None, fips = None,\n",
    "                          endpoint_url = API_REQUEST_URL, \n",
    "                          endpoint_action = API_ACTION_DAILY_SUMMARY_COUNTY, \n",
    "                          request_template = AQS_REQUEST_TEMPLATE,\n",
    "                          headers = None):\n",
    "    \n",
    "    #  This prioritizes the info from the call parameters - not what's already in the template\n",
    "    if email_address:\n",
    "        request_template['email'] = email_address\n",
    "    if key:\n",
    "        request_template['key'] = key\n",
    "    if param:\n",
    "        request_template['param'] = param\n",
    "    if begin_date:\n",
    "        request_template['begin_date'] = begin_date\n",
    "    if end_date:\n",
    "        request_template['end_date'] = end_date\n",
    "    if fips and len(fips)==5:\n",
    "        request_template['state'] = fips[:2]\n",
    "        request_template['county'] = fips[2:]            \n",
    "\n",
    "    # Make sure there are values that allow us to make a call - these are always required\n",
    "    if not request_template['email']:\n",
    "        raise Exception(\"Must supply an email address to call 'request_daily_summary()'\")\n",
    "    if not request_template['key']: \n",
    "        raise Exception(\"Must supply a key to call 'request_daily_summary()'\")\n",
    "    if not request_template['param']: \n",
    "        raise Exception(\"Must supply param values to call 'request_daily_summary()'\")\n",
    "    if not request_template['begin_date']: \n",
    "        raise Exception(\"Must supply a begin_date to call 'request_daily_summary()'\")\n",
    "    if not request_template['end_date']: \n",
    "        raise Exception(\"Must supply an end_date to call 'request_daily_summary()'\")\n",
    "    # Note we're not validating FIPS fields because not all of the daily summary actions require the FIPS numbers\n",
    "        \n",
    "    # compose the request\n",
    "    request_url = endpoint_url+endpoint_action.format(**request_template)\n",
    "        \n",
    "    # make the request\n",
    "    try:\n",
    "        # Wait first, to make sure we don't exceed a rate limit in the situation where an exception occurs\n",
    "        # during the request processing - throttling is always a good practice with a free data source\n",
    "        if API_THROTTLE_WAIT > 0.0:\n",
    "            time.sleep(API_THROTTLE_WAIT)\n",
    "        response = requests.get(request_url, headers=headers)\n",
    "        json_response = response.json()\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        json_response = None\n",
    "    return json_response\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The below function takes an existing dataframe and appends new AQI records from a provided dictionary containing AQI data. It extracts relevant fields creating a list of new rows which is then converted into a dataframe. Finally, the function concatenates this new dataframe with the original one and returns the updated dataframe, ensuring that any empty columns are removed in the process. This is to append all our API call responses into one dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to append the collected AQI data\n",
    "def concat_aqi_data(df, aqi_data):\n",
    "    # Create a list to store new rows\n",
    "    new_rows = []\n",
    "    \n",
    "    # Loop through each record in the 'Data' part of the response\n",
    "    for i in aqi_data['Data']:\n",
    "        # Create a dictionary for the new row and add it to the list\n",
    "        new_rows.append({\n",
    "            'state_code': i['state_code'],\n",
    "            'county_code': i['county_code'],\n",
    "            'site_number': i['site_number'],\n",
    "            'latitude': i['latitude'],\n",
    "            'longitude': i['longitude'],\n",
    "            'parameter_code': i['parameter_code'],\n",
    "            'parameter': i['parameter'],\n",
    "            'sample_duration': i['sample_duration'],\n",
    "            'arithmetic_mean': i['arithmetic_mean'],\n",
    "            'units_of_measure': i['units_of_measure'],\n",
    "            'date_local': i['date_local'],\n",
    "            'aqi': i['aqi']\n",
    "        })\n",
    "    \n",
    "    # Convert the list of new rows to a DataFrame and concatenate it with the existing DataFrame\n",
    "    new_df = pd.DataFrame(new_rows)\n",
    "    new_df = new_df.dropna(axis=1, how='all')\n",
    "    df = pd.concat([df, new_df], ignore_index=True)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now call API to collect AQI data for both gaseous and particulate pollutants over the date range specified in the beginning of this notebook. It initializes an empty DataFrame for each pollutant type and loops through the years to make API calls for daily summaries. If the requests are successful, the corresponding AQI data is appended to the respective DataFrames using the above function. It also prints relevent status information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No gaseous data available for 1964\n",
      "No particulate data available for 1964\n",
      "No gaseous data available for 1965\n",
      "No particulate data available for 1965\n",
      "No gaseous data available for 1966\n",
      "No particulate data available for 1966\n",
      "No gaseous data available for 1967\n",
      "No particulate data available for 1967\n",
      "No gaseous data available for 1968\n",
      "No particulate data available for 1968\n",
      "No gaseous data available for 1969\n",
      "No particulate data available for 1969\n",
      "Processing gaseous data for 1970\n",
      "No particulate data available for 1970\n",
      "Processing gaseous data for 1971\n",
      "No particulate data available for 1971\n",
      "Processing gaseous data for 1972\n",
      "No particulate data available for 1972\n",
      "Processing gaseous data for 1973\n",
      "No particulate data available for 1973\n",
      "Processing gaseous data for 1974\n",
      "No particulate data available for 1974\n",
      "Processing gaseous data for 1975\n",
      "No particulate data available for 1975\n",
      "Processing gaseous data for 1976\n",
      "No particulate data available for 1976\n",
      "Processing gaseous data for 1977\n",
      "No particulate data available for 1977\n",
      "Processing gaseous data for 1978\n",
      "No particulate data available for 1978\n",
      "Processing gaseous data for 1979\n",
      "No particulate data available for 1979\n",
      "Processing gaseous data for 1980\n",
      "No particulate data available for 1980\n",
      "Processing gaseous data for 1981\n",
      "No particulate data available for 1981\n",
      "Processing gaseous data for 1982\n",
      "No particulate data available for 1982\n",
      "Processing gaseous data for 1983\n",
      "HTTPSConnectionPool(host='aqs.epa.gov', port=443): Max retries exceeded with url: /data/api/dailyData/byCounty?email=manasars@uw.edu&key=cobaltgoose48&param=81102,88101,88502&bdate=19830501&edate=19831031&state=55&county=025 (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x169e12180>: Failed to establish a new connection: [Errno 12] Cannot allocate memory'))\n",
      "No particulate data available for 1983\n",
      "Processing gaseous data for 1984\n",
      "No particulate data available for 1984\n",
      "Processing gaseous data for 1985\n",
      "No particulate data available for 1985\n",
      "Processing gaseous data for 1986\n",
      "No particulate data available for 1986\n",
      "Processing gaseous data for 1987\n",
      "No particulate data available for 1987\n",
      "Processing gaseous data for 1988\n",
      "No particulate data available for 1988\n",
      "Processing gaseous data for 1989\n",
      "Processing particulate data for 1989\n",
      "Processing gaseous data for 1990\n",
      "Processing particulate data for 1990\n",
      "Processing gaseous data for 1991\n",
      "Processing particulate data for 1991\n",
      "Processing gaseous data for 1992\n",
      "Processing particulate data for 1992\n",
      "Processing gaseous data for 1993\n",
      "Processing particulate data for 1993\n",
      "Processing gaseous data for 1994\n",
      "Processing particulate data for 1994\n",
      "Processing gaseous data for 1995\n",
      "Processing particulate data for 1995\n",
      "Processing gaseous data for 1996\n",
      "Processing particulate data for 1996\n",
      "Processing gaseous data for 1997\n",
      "Processing particulate data for 1997\n",
      "Processing gaseous data for 1998\n",
      "Processing particulate data for 1998\n",
      "Processing gaseous data for 1999\n",
      "Processing particulate data for 1999\n",
      "Processing gaseous data for 2000\n",
      "Processing particulate data for 2000\n",
      "Processing gaseous data for 2001\n",
      "Processing particulate data for 2001\n",
      "Processing gaseous data for 2002\n",
      "Processing particulate data for 2002\n",
      "Processing gaseous data for 2003\n",
      "Processing particulate data for 2003\n",
      "Processing gaseous data for 2004\n",
      "Processing particulate data for 2004\n",
      "Processing gaseous data for 2005\n",
      "Processing particulate data for 2005\n",
      "Processing gaseous data for 2006\n",
      "Processing particulate data for 2006\n",
      "Processing gaseous data for 2007\n",
      "Processing particulate data for 2007\n",
      "Processing gaseous data for 2008\n",
      "Processing particulate data for 2008\n",
      "Processing gaseous data for 2009\n",
      "Processing particulate data for 2009\n",
      "Processing gaseous data for 2010\n",
      "Processing particulate data for 2010\n",
      "Processing gaseous data for 2011\n",
      "Processing particulate data for 2011\n",
      "Processing gaseous data for 2012\n",
      "Processing particulate data for 2012\n",
      "Processing gaseous data for 2013\n",
      "Processing particulate data for 2013\n",
      "Processing gaseous data for 2014\n",
      "Processing particulate data for 2014\n",
      "Processing gaseous data for 2015\n",
      "Processing particulate data for 2015\n",
      "Processing gaseous data for 2016\n",
      "Processing particulate data for 2016\n",
      "Processing gaseous data for 2017\n",
      "Processing particulate data for 2017\n",
      "Processing gaseous data for 2018\n",
      "Processing particulate data for 2018\n",
      "HTTPSConnectionPool(host='aqs.epa.gov', port=443): Max retries exceeded with url: /data/api/dailyData/byCounty?email=manasars@uw.edu&key=cobaltgoose48&param=42101,42401,42602,44201&bdate=20190501&edate=20191031&state=55&county=025 (Caused by NewConnectionError('<urllib3.connection.HTTPSConnection object at 0x1761fb200>: Failed to establish a new connection: [Errno 12] Cannot allocate memory'))\n",
      "No gaseous data available for 2019\n",
      "Processing particulate data for 2019\n",
      "Processing gaseous data for 2020\n",
      "Processing particulate data for 2020\n",
      "Processing gaseous data for 2021\n",
      "Processing particulate data for 2021\n",
      "Processing gaseous data for 2022\n",
      "Processing particulate data for 2022\n",
      "Processing gaseous data for 2023\n",
      "Processing particulate data for 2023\n"
     ]
    }
   ],
   "source": [
    "\n",
    "request_data = AQS_REQUEST_TEMPLATE.copy()\n",
    "request_data['email'] = USERNAME\n",
    "request_data['key'] = APIKEY\n",
    "request_data['param'] = AQI_PARAMS_GASEOUS\n",
    "request_data['state'] = CITY_LOCATIONS['madison']['fips'][:2]\n",
    "request_data['county'] = CITY_LOCATIONS['madison']['fips'][2:]\n",
    "\n",
    "gaseous_responses = []\n",
    "particulate_responses = []\n",
    "\n",
    "# Initialize an empty DataFrame for the AQI data\n",
    "gaseous_aqi_df = pd.DataFrame(columns=['state_code', 'county_code','site_number', 'latitude', 'longitude', 'parameter_code', \n",
    "                                       'parameter','sample_duration','arithmetic_mean', 'units_of_measure','date_local', 'aqi'])\n",
    "particulate_aqi_df = pd.DataFrame(columns=['state_code', 'county_code','site_number', 'latitude', 'longitude', 'parameter_code', \n",
    "                                           'parameter','sample_duration','arithmetic_mean', 'units_of_measure', 'date_local', 'aqi'])\n",
    "\n",
    "# Loop through the years and request data\n",
    "for year in range(STARTYEAR, ENDYEAR):\n",
    "    begin_date = f\"{year}0501\"  # May 1st of the given year\n",
    "    end_date = f\"{year}1031\"    # October 31st of the given year\n",
    "\n",
    "    # Request gaseous data\n",
    "    request_data['param'] = AQI_PARAMS_GASEOUS\n",
    "    gaseous_responses = request_daily_summary(request_template=request_data, begin_date=begin_date, end_date=end_date)\n",
    "    if gaseous_responses and gaseous_responses[\"Header\"][0]['status'] == \"Success\":\n",
    "        print(f\"Processing gaseous data for {year}\")\n",
    "        gaseous_aqi_df = concat_aqi_data(gaseous_aqi_df, gaseous_responses)\n",
    "    else:\n",
    "        print(f\"No gaseous data available for {year}\")\n",
    "\n",
    "    # Request particulate data\n",
    "    request_data['param'] = AQI_PARAMS_PARTICULATES\n",
    "    particulate_responses = request_daily_summary(request_template=request_data, begin_date=begin_date, end_date=end_date)\n",
    "    if particulate_responses and particulate_responses[\"Header\"][0]['status'] == \"Success\":\n",
    "        print(f\"Processing particulate data for {year}\")\n",
    "        particulate_aqi_df = concat_aqi_data(particulate_aqi_df, particulate_responses)\n",
    "    else:\n",
    "        print(f\"No particulate data available for {year}\")\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see there is no data available from 1964 - 1969. But we have gaseous data available from 1970 and particles data avalable only from 1989."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some pollutant parameter data is reported hourly or multiple times per day, potentially leading to duplicate records when values do not change. To prevent this, we will remove duplicates before saving the data to two csv files - [gaseous_aqi_1964_2024.csv](https://github.com/ManasaSRonur/data-512-project/blob/main/intermediary_files/gaseous_aqi_1964_2024.csv), [particulate_aqi_1964_2024.csv](https://github.com/ManasaSRonur/data-512-project/blob/main/intermediary_files/particulate_aqi_1964_2024.csv). These files now have just raw responses from API calls and can be loaded as and when needed for analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gaseous AQI Data saved to intermediary_files/gaseous_aqi_1964_2024.csv\n",
      "Particulate AQI Data saved to intermediary_files/particulate_aqi_1964_2024.csv\n"
     ]
    }
   ],
   "source": [
    "gaseous_aqi_df = gaseous_aqi_df.drop_duplicates()\n",
    "gaseous_aqi_df.to_csv(\"intermediary_files/gaseous_aqi_1964_2024.csv\", index=False)\n",
    "print(\"Gaseous AQI Data saved to intermediary_files/gaseous_aqi_1964_2024.csv\")\n",
    "\n",
    "particulate_aqi_df = particulate_aqi_df.drop_duplicates()\n",
    "particulate_aqi_df.to_csv(\"intermediary_files/particulate_aqi_1964_2024.csv\", index=False)\n",
    "print(\"Particulate AQI Data saved to intermediary_files/particulate_aqi_1964_2024.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 2: Data Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will load the previously generated CSV files and combine them into a single dataframe for further analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total records in AQI dataframe is 74718\n",
      "Total Records with blank AQI is 35625\n"
     ]
    }
   ],
   "source": [
    "gaseous_df = pd.read_csv('intermediary_files/gaseous_aqi_1964_2024.csv')\n",
    "particulate_df = pd.read_csv('intermediary_files/particulate_aqi_1964_2024.csv')\n",
    "\n",
    "aqi_df = pd.concat([gaseous_df, particulate_df], ignore_index=True)\n",
    "\n",
    "print(\"Total records in AQI dataframe is\",len(aqi_df))\n",
    "\n",
    "print(\"Total Records with blank AQI is\",aqi_df['aqi'].isna().sum())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nearly half the records do no have AQI data. However we do have gas and particles data available for each record. We will further anlayze if we can use this to calculate AQI values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "parameter                               sample_duration        \n",
       "Ozone                                   8-HR RUN AVG BEGIN HOUR    18699\n",
       "                                        1 HOUR                      9436\n",
       "Sulfur dioxide                          1 HOUR                      7300\n",
       "                                        3-HR BLK AVG                7287\n",
       "                                        24-HR BLK AVG               7093\n",
       "PM2.5 - Local Conditions                1 HOUR                      4279\n",
       "                                        24-HR BLK AVG               4254\n",
       "Carbon monoxide                         8-HR RUN AVG END HOUR       3232\n",
       "                                        1 HOUR                      3230\n",
       "PM2.5 - Local Conditions                24 HOUR                     2781\n",
       "Sulfur dioxide                          5 MINUTE                    1655\n",
       "PM10 Total 0-10um STP                   24 HOUR                     1092\n",
       "                                        1 HOUR                       939\n",
       "                                        24-HR BLK AVG                933\n",
       "Sulfur dioxide                          24 HOUR                      847\n",
       "Acceptable PM2.5 AQI & Speciation Mass  1 HOUR                       812\n",
       "                                        24-HR BLK AVG                801\n",
       "Nitrogen dioxide (NO2)                  24 HOUR                       48\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "aqi_df.value_counts(subset = ['parameter', 'sample_duration'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The [Technical Assistance Document for the Reporting of Daily Air Quality  the Air Quality Index (AQI)](https://www.airnow.gov/publications/air-quality-index/technical-assistance-document-for-reporting-the-daily-aqi/) in the airnow website has detailed steps to calculate AQI information. A little knowledge from the web on AQI calculation along with this document will serve as base for our AQI calculation.\n",
    "\n",
    "First step is to define breakpoints with concentration ranges and AQI ranges for each of the 6 pollutants identified above. Breakpoints are in the form of (bp_low, bp_high, i_low, i_high) where bp_low and bp_high are the concentration range in which the pollutant level falls , i_low and i_high are the corresponding AQI range for that concentration level. Next for each record in the dataframe, we retrieve the pollutant concentration value and check if this is in one of the defined breakpoint ranges. If yes we calculate AQI as follows and rounded it to nearest value to match with the exisitng value type.\n",
    "\n",
    "AQI = (bp_high  bp_low / i_high  i_low)  (concentration  bp_low) + i_low"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the breakpoints for each pollutant\n",
    "breakpoints = {\n",
    "    'Carbon monoxide': [\n",
    "        (0, 4.4, 0, 50),\n",
    "        (4.5, 9.4, 51, 100),\n",
    "        (9.5, 12.4, 101, 150),\n",
    "        (12.5, 15.4, 151, 200),\n",
    "        (15.5, 30.4, 201, 300),\n",
    "        (30.5, 50.4, 301, 500)\n",
    "    ],\n",
    "    'Sulfur dioxide': [\n",
    "        (0, 35, 0, 50),\n",
    "        (36, 75, 51, 100),\n",
    "        (76, 185, 101, 150),\n",
    "        (186, 304, 151, 200),\n",
    "        (305, 604, 201, 300),\n",
    "        (605, 1004, 301, 500)\n",
    "    ],\n",
    "    'Nitrogen dioxide (NO2)': [\n",
    "        (0, 53, 0, 50),\n",
    "        (54, 100, 51, 100),\n",
    "        (101, 360, 101, 150),\n",
    "        (361, 649, 151, 200),\n",
    "        (650, 1249, 201, 300),\n",
    "        (1250, 2049, 301, 500)\n",
    "    ],\n",
    "    'Ozone': [\n",
    "        (0, 0.054, 0, 50),\n",
    "        (0.055, 0.070, 51, 100),\n",
    "        (0.071, 0.085, 101, 150),\n",
    "        (0.086, 0.105, 151, 200),\n",
    "        (0.106, 0.200, 201, 300),\n",
    "        (0.201, 0.604, 301, 500)\n",
    "    ],\n",
    "    'PM10 Total 0-10um STP': [\n",
    "        (0, 54, 0, 50),\n",
    "        (54, 154, 51, 100),\n",
    "        (155, 254, 101, 150),\n",
    "        (255, 354, 151, 200),\n",
    "        (355, 424, 201, 300),\n",
    "        (425, 604, 301, 500)\n",
    "    ],\n",
    "    'PM2.5 - Local Conditions': [\n",
    "        (0, 12.00, 0, 50),\n",
    "        (12.01, 35.4, 51, 100),\n",
    "        (35.5, 55.4, 101, 150),\n",
    "        (55.5, 150.4, 151, 200),\n",
    "        (150.5, 250.4, 201, 300),\n",
    "        (250.5, 500.4, 301, 500)\n",
    "    ]\n",
    "}\n",
    "\n",
    "\n",
    "def calculate_aqi(row):\n",
    "    pollutant = row['parameter']\n",
    "    concentration = round(row['arithmetic_mean'], 2)  # Round to 2 decimal places\n",
    "\n",
    "    \n",
    "    # Identify breakpoints for the pollutant\n",
    "    bp_data = breakpoints.get(pollutant)\n",
    "    if not bp_data:\n",
    "        return None  # Skip if no breakpoints defined\n",
    "    \n",
    "    # Find the corresponding AQI range\n",
    "    for (bp_low, bp_high, i_low, i_high) in bp_data:\n",
    "        if bp_low <= concentration <= bp_high:\n",
    "            # Calculate AQI using the formula\n",
    "            aqi = ((i_high - i_low) / (bp_high - bp_low)) * (concentration - bp_low) + i_low\n",
    "            return round(aqi)\n",
    "    return None  # Return None if no matching range\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now apply our calculation on the records that have missing AQI values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the function only if 'aqi' is blank (NaN), otherwise keep the existing 'aqi' value\n",
    "aqi_df['aqi'] = aqi_df.apply(lambda row: calculate_aqi(row) if pd.isna(row['aqi']) else row['aqi'], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This must have fixed the records with blank AQI values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Records with blank AQI is 1705\n"
     ]
    }
   ],
   "source": [
    "print(\"Total Records with blank AQI is\",aqi_df['aqi'].isna().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We still have 1705 records without AQI values, this could be due to concentration value out of defined range. Upon investigation I found these are the records with negative concentration of sulphur dioxde which is clealry a data issue. so we will drop these records."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "aqi_df = aqi_df.dropna(subset=['aqi'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we will have to ensure the data we have obtained is from the stations relevant to Madison, so we can reliably use to this to compare smoke data in and around Madison. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The stations that returned AQI information are: [  1   2   3   5   8   9 999 898 993 994 998  21  22   7  26  27  31  34\n",
      "  41  42  47  37  25  48]\n"
     ]
    }
   ],
   "source": [
    "print(f\"The stations that returned AQI information are: {aqi_df['site_number'].unique()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us analyse the location of these stations, by calculating the proximity of these stations from Madison using the [EPSG:4326](https://www.google.com/url?q=https%3A%2F%2Fepsg.io%2F4326) coordinate system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    site_number   latitude  longitude  count  distance_from_madison\n",
      "0            41  43.101010 -89.357680  31145               2.255870\n",
      "1            27  43.067218 -89.398452   9566               0.889371\n",
      "2            26  43.117495 -89.362618   8804               3.147467\n",
      "3            47  43.073300 -89.435800   8754               2.614796\n",
      "4            22  43.216381 -89.333173   2698              10.112936\n",
      "5            31  43.091940 -89.359007   2654               1.741661\n",
      "6            42  43.089996 -89.358729   1818               1.663867\n",
      "7            34  43.246935 -89.335116   1786              12.143819\n",
      "8            21  43.090829 -89.357896   1415               1.732996\n",
      "9             7  43.026941 -89.422620   1268               3.829968\n",
      "10           25  43.081940 -89.376786    522               0.622710\n",
      "11           37  43.043054 -89.248727    481               7.195885\n",
      "12          993  43.114172 -89.360909    439               2.966579\n",
      "13          898  43.072227 -89.382021    413               0.203612\n",
      "14            1  43.072218 -89.382063    254               0.203015\n",
      "15            5  43.068052 -89.405119    170               1.156000\n",
      "16          994  43.090005 -89.362298    165               1.528884\n",
      "17            3  43.118884 -89.363729    157               3.219094\n",
      "18            2  43.092505 -89.335353    110               2.758148\n",
      "19          999  43.133338 -89.333408    106               4.792138\n",
      "20            8  43.106106 -89.357618    102               2.549061\n",
      "21            9  43.081107 -89.373452    101               0.698758\n",
      "22           48  43.092996 -89.340168     62               2.558822\n",
      "23          998  43.106949 -89.336743     23               3.271957\n"
     ]
    }
   ],
   "source": [
    "site_distance = aqi_df.value_counts(subset=['site_number','latitude', 'longitude']).reset_index()\n",
    "geodcalc = Geod(ellps='WGS84')\n",
    "madison = CITY_LOCATIONS['madison']['latlon']\n",
    "# get distance in miles away from city\n",
    "site_distance['distance_from_madison'] = site_distance.apply(lambda x: \n",
    "    geodcalc.inv(madison[1],madison[0],x['longitude'],x['latitude'])[2]*0.00062137\n",
    "    , axis=1)\n",
    "print(site_distance)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Most stations are within 5 miles of madison except for station 37, 22 and 34 that are 7, 10 and 12 miles away. so the data from all of these stations can be effectively used in our analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 3: File Generation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now have clean and processed data, so we will aggregate this data to obtain average AQI for each year bewteen 1st of May to 31st of Oct. This data will be saved as csv, which we will later use for analysis especially to validate our smoke estimation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert date to datetime and extract year\n",
    "aqi_df['date_local'] = pd.to_datetime(aqi_df['date_local'])\n",
    "aqi_df['year'] = aqi_df['date_local'].dt.year\n",
    "\n",
    "# Calculate the mean AQI per year based on the daily maximum values\n",
    "average_aqi_per_year = aqi_df.groupby('year')['aqi'].mean().reset_index()\n",
    "\n",
    "average_aqi_per_year.to_csv('intermediary_files/yearly_aqi_1964_2024.csv', index=False)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "anything",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
